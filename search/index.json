[{"content":"Redis介绍 一、核心设计思想 内存优先 所有数据存储在内存中，读写性能极高（可达10万+ QPS）。 通过持久化机制将数据异步保存到磁盘，保证数据可靠性。 单线程模型（Redis 6.0前核心逻辑） 使用单线程处理命令，避免多线程竞争和锁开销。 基于非阻塞I/O多路复用（epoll/kqueue）处理高并发连接。 6.0+版本引入多线程处理网络I/O，但核心命令执行仍为单线程。 二、数据存储原理 1. 键值存储结构 所有数据以**字典（哈希表）**形式组织，键为字符串，值支持多种数据结构。 全局哈希表保存所有键的指针，通过哈希函数快速定位（O(1)时间复杂度）。 2. 高效数据结构 Redis的值不仅支持简单字符串，还提供以下高级结构：\nString：动态字符串（SDS），可存储文本、数字或二进制数据。 List：双向链表或压缩列表（ziplist），支持队列/栈操作。 Hash：类似字典结构，存储字段-值映射。 Set：无序集合，基于哈希表实现。 Sorted Set：有序集合，使用跳表（skiplist） + 哈希表实现范围查询。 HyperLogLog：概率数据结构，用于基数统计。 Stream：日志流，支持消息队列场景。 3. 内存优化机制 渐进式Rehash：扩容时逐步迁移数据，避免长时间阻塞。 压缩存储：小数据使用ziplist/intset等紧凑结构，减少内存占用。 过期键清理：定期删除+惰性删除结合。 三、持久化原理 Redis提供两种持久化方式：\n1. RDB（快照） 定时将内存数据全量保存为二进制文件（.rdb）。 使用COW（Copy-On-Write） 技术：fork子进程写盘，父进程继续服务。 优点：文件紧凑，恢复速度快；缺点：可能丢失最后一次快照后的数据。 2. AOF（追加日志） 记录所有写操作命令到日志文件，重启时重放命令恢复数据。 支持配置同步策略： appendfsync always：每次写都同步，数据最安全但性能低。 appendfsync everysec：每秒同步（默认）。 appendfsync no：由操作系统决定。 AOF重写：定期压缩日志，去除冗余命令。 3. 混合持久化（Redis 4.0+） 结合RDB和AOF，先写RDB快照，再追加增量AOF日志，兼顾恢复速度和数据安全。 四、高可用与集群 1. 主从复制 主节点异步复制数据到从节点，支持读写分离和故障转移。 全量复制（首次同步）使用RDB + 缓冲命令流；增量复制通过复制偏移量实现。 2. 哨兵（Sentinel） 监控主节点状态，自动故障转移和切换。 通过投票机制避免脑裂问题。 3. 集群模式（Redis Cluster） 数据分片：采用哈希槽（hash slot，共16384个槽）分散数据到多个节点。 去中心化架构：每个节点保存集群状态信息，通过Gossip协议通信。 故障检测与转移：主节点故障时，从节点自动接替。 五、性能关键点 纯内存操作：避免磁盘I/O瓶颈。 单线程模型：避免上下文切换和锁竞争。 高效数据结构：针对场景优化存储和访问。 I/O多路复用：单线程处理大量并发连接。 管道（Pipeline）：批量发送命令，减少网络往返时间。 六、典型应用场景 缓存：加速热点数据访问，支持过期策略。 会话存储：存储用户会话信息。 排行榜：使用Sorted Set实现实时排名。 消息队列：使用List/Stream实现轻量队列。 分布式锁：通过SETNX命令实现。 七、注意事项 内存限制：数据量受物理内存限制，需监控内存使用。 持久化权衡：根据对数据安全性的要求选择RDB/AOF策略。 集群管理：Redis Cluster需客户端支持重定向，运维复杂度较高。 缓存穿透/雪崩：需结合布隆过滤器、过期时间分散等策略防护。 总结 Redis通过内存存储+高效数据结构+异步持久化的设计，在性能与功能间取得平衡。其单线程模型简化了并发控制，而集群方案提供了横向扩展能力。理解其原理有助于根据业务场景合理选择数据结构、持久化策略和集群架构。\n","date":"2022-03-14T15:23:16Z","image":"https://sturdy-space-chainsaw-595gwwvqgjvf4vpx-1313.app.github.dev/redis.jpeg","permalink":"https://sturdy-space-chainsaw-595gwwvqgjvf4vpx-1313.app.github.dev/p/redis%E4%BB%8B%E7%BB%8D/","title":"Redis介绍"},{"content":"Redis 不同部署模式的使用场景 Redis 支持三种主要部署模式：单机模式、哨兵模式(Sentinel)和集群模式(Cluster)。每种模式适合不同的业务场景，下面是它们的详细对比和使用建议。\n单机模式 (Standalone) 特点 单个 Redis 实例运行 无自动故障转移能力 配置简单，资源消耗少 适用场景 开发/测试环境：快速搭建，低成本 小型应用：数据量小(小于2GB)，访问量低(QPS \u0026lt; 1万) 缓存场景：可以接受短暂不可用 本地缓存替代：比本地缓存更强大的数据结构支持 不适用场景 生产环境关键业务(除非可以接受停机) 大数据量(超过单机内存容量) 高可用性要求的场景 哨兵模式 (Sentinel) 特点 由1个主节点和多个从节点组成 哨兵节点监控主从状态 自动故障转移(主节点宕机时选举新主) 客户端自动发现可用节点 适用场景 读写分离：读多写少场景(主写从读) 高可用需求：需要自动故障转移 中小规模数据：数据量适合单机存储 业务连续性要求高：如电商库存、支付系统 典型配置 至少3个哨兵节点(避免脑裂) 1主2从是常见配置 集群模式 (Cluster) 特点 数据分片(16384个slot)存储在多节点 每个分片有主从复制 自动故障转移 支持水平扩展 适用场景 大数据量：超过单机内存容量(如100GB+) 高性能需求：需要分散读写压力 高并发场景：QPS超过单机处理能力(10万+) 需要线性扩展：随业务增长方便扩容 不适用场景 事务操作多的场景(仅支持同一节点上的事务) 大量使用多键操作(除非所有键在同一节点) 模式对比表 特性\t单机模式\t哨兵模式\t集群模式 数据量\t\u0026lt; 2GB\t\u0026lt; 64GB(单机最大内存)\t理论上无限 高可用\t不支持\t支持\t支持 扩展性\t垂直扩展\t读写分离\t水平扩展 性能\t单机性能\t读性能可扩展\t读写性能均可扩展 复杂度\t简单\t中等\t复杂 适用场景\t开发/测试/小型\t中等规模生产环境\t大规模生产环境 客户端支持\t所有客户端\t需要支持Sentinel\t需要支持Cluster 选型建议 开发测试：单机模式足够 中小型生产环境： ○ 如果主要是缓存用途 → 单机模式+持久化 ○ 如果需要高可用 → 哨兵模式(1主2从+3哨兵) 大型生产环境： ○ 数据量大 → 集群模式 ○ 超高并发 → 集群模式 特殊场景： ○ 需要地理冗余 → 哨兵模式跨机房部署 ○ 需要极高写入性能 → 集群模式分散写入压力 配置示例 哨兵模式典型配置 shell 复制 下载 启动3个哨兵 redis-sentinel sentinel1.conf redis-sentinel sentinel2.conf redis-sentinel sentinel3.conf\n哨兵配置示例 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 5000 sentinel failover-timeout mymaster 60000 集群模式最小配置 shell 复制 下载\n至少需要3主3从 redis-server cluster-node1.conf \u0026ndash;cluster-enabled yes redis-server cluster-node2.conf \u0026ndash;cluster-enabled yes redis-server cluster-node3.conf \u0026ndash;cluster-enabled yes\n\u0026hellip;从节点配置类似 创建集群 redis-cli \u0026ndash;cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 根据业务需求、数据规模和团队运维能力选择合适的Redis部署模式，可以在性能、可用性和复杂度之间取得平衡。\n","date":"2022-03-16T11:30:24Z","image":"https://sturdy-space-chainsaw-595gwwvqgjvf4vpx-1313.app.github.dev/redis.jpeg","permalink":"https://sturdy-space-chainsaw-595gwwvqgjvf4vpx-1313.app.github.dev/p/%E4%B8%8D%E5%90%8C%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","title":"不同部署模式的使用场景"},{"content":"Redis生产事件总结报告 事件概述 事件时间 触发时间：2025年4月 恢复时间：2025年4月(当天) 持续时间：约1小时 影响范围 系统影响：Redis集群不可用 业务影响：依赖Redis的相关金融服务功能受影响 应用影响：全部应用重启时无法正常启动 事件背景 历史操作（2022年） 2022年进行的Redis集群切换演练中，由于操作配置问题，导致两个主节点（master）被部署在同一数据中心。这一配置隐患一直存在但未暴露，直到本次事件触发。\n事件触发条件 2025年4月，该数据中心发生故障，导致位于同一中心的两个master节点同时不可用。根据Redis集群的故障转移机制：\n一半的slot（哈希槽）正常完成了自动转移 另一半slot因两个master在同一位置，无法满足故障转移条件，导致迁移失败 影响分析 直接表现 集群状态异常：部分数据分片不可用 应用启动失败：所有应用程序重启时均无法正常启动 服务不可用：依赖Redis的业务功能受限或不可用 配置分析 关键配置：cluster-require-full-coverage=yes\n配置含义：要求集群所有slot都必须被覆盖时，集群才对外提供写服务 金融系统要求：银行金融系统因强一致性要求，必须设置此参数为yes 问题表现：当部分slot不可用时，整个集群拒绝写操作，导致服务不可用 根本原因分析 第一层原因：架构配置缺陷 违反部署原则：两个master节点部署在同一数据中心，违反高可用部署原则 历史遗留隐患：2022年演练后的配置未恢复，隐患长期存在 第二层原因：客户端框架限制 自研框架中Jedis 2.9版本存在以下问题：\n问题项 具体表现 影响程度 拓扑自动刷新 不支持自动刷新集群拓扑 高 启动连接策略 启动时连接所有配置节点 严重 故障容忍度 遇到故障节点直接失败 严重 启动流程缺陷：\ngraph TD\rA[应用启动] --\u003e B[Jedis初始化]\rB --\u003e C[连接所有配置节点]\rC --\u003e D{遇到故障节点?}\rD --\u003e|是| E[连接失败]\rD --\u003e|否| F[启动成功]\rE --\u003e G[启动终止] 初始化槽位缓存时 JedisClusterConnectionHandler中源码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 private void initializeSlotsCache(Set\u0026lt;HostAndPort\u0026gt; startNodes, GenericObjectPoolConfig poolConfig, String password) { for (HostAndPort hostAndPort : startNodes) { Jedis jedis = new Jedis(hostAndPort.getHost(), hostAndPort.getPort()); if (password != null) { jedis.auth(password); } try { cache.discoverClusterNodesAndSlots(jedis); break; } catch (JedisConnectionException e) { // try next nodes } finally { if (jedis != null) { jedis.close(); } } } } 每一个节点都会在auth时进行contect连接，也就导致了启动时连接故障节点抛出JedisConnectionException最终启动失败。\n第三层原因：集群配置与业务要求的矛盾 业务要求：强一致性 → cluster-require-full-coverage=yes 可用性要求：部分故障时仍需服务 → 与上述配置矛盾 缺乏应用中间方案：部分功能未设计redis集群故障时的降级策略 解决方案与修复过程 第一阶段：紧急恢复（应用启动） 方案：修改应用启动健康检查机制\n健康检查过滤：启动时跳过配置中的故障节点 实现方式： 在Jedis初始化前增加节点健康检查 仅连接健康的节点完成初始化 记录故障节点信息用于后续恢复 修复代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.stereotype.Component; import java.util.HashSet; import java.util.Set; import java.util.concurrent.CopyOnWriteArraySet; /** * @author huqinuo * @version 1.0 * @description Redis节点上下文管理器 * @data 2025/12/1 9:25 */ @Component public class RedisNodeContext { private final Logger log = LoggerFactory.getLogger(RedisNodeContext.class); private final Set\u0026lt;String\u0026gt; unhealthyNodes = new CopyOnWriteArraySet\u0026lt;\u0026gt;(); private final Set\u0026lt;String\u0026gt; allNodes = new CopyOnWriteArraySet\u0026lt;\u0026gt;(); /** * 初始化方法，从静态数据加载 */ public void initializeFromStaticData() { if (RedisHealthChecker.isHealthCheckCompleted()) { this.allNodes.clear(); this.allNodes.addAll(RedisHealthChecker.getAllNodes()); this.unhealthyNodes.clear(); this.unhealthyNodes.addAll(RedisHealthChecker.getUnhealthyNodes()); log.info(\u0026#34;RedisNodeContext initialized from static data: {}/{} nodes healthy\u0026#34;, getHealthyNodeCount(), allNodes.size()); } } /** * 添加不健康节点 */ public void addUnhealthyNode(String node) { unhealthyNodes.add(node); } /** * 移除不健康节点（节点恢复时调用） */ public void removeUnhealthyNode(String node) { unhealthyNodes.remove(node); } /** * 获取所有不健康节点 */ public Set\u0026lt;String\u0026gt; getUnhealthyNodes() { return new HashSet\u0026lt;\u0026gt;(unhealthyNodes); } /** * 检查节点是否不健康 */ public boolean isNodeUnhealthy(String node) { return unhealthyNodes.contains(node); } /** * 设置所有节点 */ public void setAllNodes(Set\u0026lt;String\u0026gt; nodes) { allNodes.clear(); allNodes.addAll(nodes); } /** * 获取所有节点 */ public Set\u0026lt;String\u0026gt; getAllNodes() { return new HashSet\u0026lt;\u0026gt;(allNodes); } /** * 添加单个节点 */ public void addNode(String node) { allNodes.add(node); } /** * 获取健康节点数量 */ public int getHealthyNodeCount() { return allNodes.size() - unhealthyNodes.size(); } /** * 获取不健康节点数量 */ public int getUnhealthyNodeCount() { return unhealthyNodes.size(); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package open.ref.redis; import 自研包.RedisFactory; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.DependsOn; import org.springframework.scheduling.annotation.EnableScheduling; /** * @author huqinuo * @version 1.0 * @description Redis配置类(健康检查使用) * @data 2025/12/1 15:31 */ @Configuration @EnableScheduling public class RedisClusterConfig { /** * redis节点上下文 */ @Bean @DependsOn(\u0026#34;redisHealthChecker\u0026#34;) // 确保在健康检查后执行 public RedisNodeContext redisNodeContext() { RedisNodeContext context = new RedisNodeContext(); context.initializeFromStaticData(); // 从静态数据初始化 return context; } /** * 拓扑刷新器 */ @Bean @DependsOn(\u0026#34;redisNodeContext\u0026#34;) public RedisClusterTopologyRefresher redisClusterTopologyRefresher(@Autowired RedisNodeContext redisNodeContext, RedisFactory redisFactory) { return new RedisClusterTopologyRefresher(redisNodeContext,redisFactory); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 package open.ref.redis; import open.svc.dict.Const; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.BeansException; import org.springframework.beans.factory.config.BeanFactoryPostProcessor; import org.springframework.beans.factory.config.ConfigurableListableBeanFactory; import org.springframework.context.EnvironmentAware; import org.springframework.core.env.ConfigurableEnvironment; import org.springframework.core.env.Environment; import org.springframework.core.env.MapPropertySource; import org.springframework.stereotype.Component; import redis.clients.jedis.Jedis; import java.util.*; /** * @author huqinuo * @version 1.0 * @description Redis集群连接健康检查 * @data 2025/12/1 10:02 */ @Component public class RedisHealthChecker implements BeanFactoryPostProcessor, EnvironmentAware { private ConfigurableEnvironment environment; private final Logger log = LoggerFactory.getLogger(RedisHealthChecker.class); private static final Set\u0026lt;String\u0026gt; unhealthyNodes = new HashSet\u0026lt;\u0026gt;(); private static final Set\u0026lt;String\u0026gt; allNodes = new HashSet\u0026lt;\u0026gt;(); private static final int CONNECTION_TIMEOUT = 5000; private static boolean healthCheckCompleted = false; @Override public void setEnvironment(Environment environment) { this.environment = (ConfigurableEnvironment) environment; } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { if (healthCheckCompleted) { return; // 避免重复执行 } if (!isRedisClusterEnabled()) { log.info(\u0026#34;Redis cluster is not enabled, skip health check\u0026#34;); healthCheckCompleted = true; return; } String oriRedisNodeUrl = getOriRedisNodeUrl(); if (Objects.isNull(oriRedisNodeUrl)) { log.warn(\u0026#34;Application does not have Redis configuration\u0026#34;); healthCheckCompleted = true; return; } // 执行健康检查并更新配置 performInitialHealthCheck(oriRedisNodeUrl); healthCheckCompleted = true; // 注册静态上下文到BeanFactory registerNodeContextBean(beanFactory); } /** * 注册节点上下文到BeanFactory */ private void registerNodeContextBean(ConfigurableListableBeanFactory beanFactory) { RedisNodeContext nodeContext = new RedisNodeContext(); nodeContext.setAllNodes(new HashSet\u0026lt;\u0026gt;(allNodes)); for (String node : unhealthyNodes) { nodeContext.addUnhealthyNode(node); } beanFactory.registerSingleton(\u0026#34;redisNodeContext\u0026#34;, nodeContext); log.info(\u0026#34;RedisNodeContext registered as singleton bean\u0026#34;); } /** * 执行初始健康检查 */ private void performInitialHealthCheck(String oriRedisNodeUrl) { // 保存所有原始节点 allNodes.clear(); allNodes.addAll(Arrays.asList(oriRedisNodeUrl.split(Const.Symbol.Comma))); log.info(\u0026#34;Starting initial Redis cluster health check for {} nodes\u0026#34;, allNodes.size()); // 过滤健康节点 List\u0026lt;String\u0026gt; healthyNodes = filterHealthyClusterNodes(oriRedisNodeUrl); if (healthyNodes.isEmpty()) { log.info(\u0026#34;No healthy Redis cluster nodes available\u0026#34;); return; } String healthyRedisNodeUrl = String.join(Const.Symbol.Comma, healthyNodes); // 如果有不健康节点，更新配置 if (!oriRedisNodeUrl.equals(healthyRedisNodeUrl)) { updateRedisConfiguration(healthyRedisNodeUrl); log.warn(\u0026#34;Redis configuration updated due to unhealthy nodes. Healthy nodes: {}\u0026#34;, healthyRedisNodeUrl); } // 记录初始健康状态 log.info(\u0026#34;Initial Redis cluster health status: {}/{} nodes healthy\u0026#34;, (allNodes.size() - unhealthyNodes.size()), allNodes.size()); log.info(\u0026#34;Unhealthy nodes: {}\u0026#34;, unhealthyNodes); } private Boolean isRedisClusterEnabled() { String isEnabled = environment.getProperty(\u0026#34;自研包.cluster.enabled\u0026#34;); if (isEnabled != null) { return Boolean.parseBoolean(isEnabled); } return Boolean.FALSE; } private String getOriRedisNodeUrl() { // 提取集群配置 String clusterNodes = environment.getProperty(\u0026#34;spring.redis.cluster.nodes\u0026#34;); log.info(\u0026#34;Redis cluster nodes configuration: {}\u0026#34;, clusterNodes); if (clusterNodes != null) { return clusterNodes; } return null; } private String getAuthPassword() { // 提取认证配置 String auth = environment.getProperty(\u0026#34;spring.redis.password\u0026#34;); if (auth != null) { return auth; } return null; } private List\u0026lt;String\u0026gt; filterHealthyClusterNodes(String oriRedisNodeUrl) { List\u0026lt;String\u0026gt; healthyNodes = new ArrayList\u0026lt;\u0026gt;(); String[] clusterNodes = oriRedisNodeUrl.split(Const.Symbol.Comma); for (String node : clusterNodes) { if (checkRedisNode(node)) { healthyNodes.add(node); log.info(\u0026#34;Redis cluster node {} is healthy\u0026#34;, node); } else { // 节点不健康，记录到静态集合 unhealthyNodes.add(node); log.warn(\u0026#34;Redis cluster node {} is unhealthy, added to monitoring list\u0026#34;, node); } } return healthyNodes; } private boolean checkRedisNode(String node) { String[] parts = node.split(\u0026#34;:\u0026#34;); String host = parts[0]; int port = parts.length \u0026gt; 1 ? Integer.parseInt(parts[1]) : 6379; try (Jedis jedis = new Jedis(host, port, CONNECTION_TIMEOUT)) { String auth = getAuthPassword(); if (!Objects.isNull(auth)) { jedis.auth(auth); } String result = jedis.ping(); return \u0026#34;PONG\u0026#34;.equals(result); } catch (Exception e) { log.warn(\u0026#34;Redis node {}:{} check failed: {}\u0026#34;, host, port, e.getMessage()); return false; } } private void updateRedisConfiguration(String healthyRedisNodeUrl) { Map\u0026lt;String, Object\u0026gt; updatedProperties = new HashMap\u0026lt;\u0026gt;(); // 更新集群配置 if (healthyRedisNodeUrl != null) { updatedProperties.put(\u0026#34;athena.redis.url\u0026#34;, healthyRedisNodeUrl); updatedProperties.put(\u0026#34;spring.redis.cluster.nodes\u0026#34;, healthyRedisNodeUrl); } // 将更新后的配置添加到Environment的最高优先级 environment.getPropertySources().addFirst( new MapPropertySource(\u0026#34;redisHealthProperties\u0026#34;, updatedProperties) ); log.info(\u0026#34;Redis configuration has been updated with healthy nodes: {}\u0026#34;, healthyRedisNodeUrl); } /** * 静态方法供其他组件访问 */ public static Set\u0026lt;String\u0026gt; getUnhealthyNodes() { return new HashSet\u0026lt;\u0026gt;(unhealthyNodes); } public static Set\u0026lt;String\u0026gt; getAllNodes() { return new HashSet\u0026lt;\u0026gt;(allNodes); } public static boolean isHealthCheckCompleted() { return healthCheckCompleted; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 package open.ref.redis; import 自研包.JedisFactoryClusterImpl; import 自研包.RedisFactory; import open.svc.dict.Const; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Value; import org.springframework.scheduling.annotation.Scheduled; import org.springframework.stereotype.Component; import redis.clients.jedis.BinaryJedisCluster; import redis.clients.jedis.Jedis; import redis.clients.jedis.JedisCluster; import redis.clients.jedis.JedisSlotBasedConnectionHandler; import java.lang.reflect.Field; import java.util.*; /** * @author huqinuo * @version 1.0 * @description Redis集群拓扑刷新器 * @data 2025/12/1 13:41 */ @Component public class RedisClusterTopologyRefresher { private final Logger log = LoggerFactory.getLogger(RedisClusterTopologyRefresher.class); private final RedisNodeContext redisNodeContext; private final RedisFactory redisFactory; @Value(\u0026#34;${自研包密码}\u0026#34;) private String auth; /** * 2分钟刷新一次 */ private static final long REFRESH_INTERVAL = 2 * 60 * 1000; /** * 连接超时时间 */ private static final int CONNECTION_TIMEOUT = 5000; /** * 是否刷新中 */ private volatile boolean isRefreshing = false; @Autowired public RedisClusterTopologyRefresher(RedisNodeContext redisNodeContext, RedisFactory redisFactory) { this.redisNodeContext = redisNodeContext; this.redisFactory = redisFactory; log.info(\u0026#34;RedisClusterTopologyRefresher initialized with {} unhealthy nodes\u0026#34;, redisNodeContext.getUnhealthyNodeCount()); } /** * 定时刷新不健康节点的拓扑信息 */ @Scheduled(fixedRate = REFRESH_INTERVAL) public void refreshTopologyForUnhealthyNodes() { if (isRefreshing) { log.info(\u0026#34;Topology refresh is already in progress, skip this execution\u0026#34;); return; } //从单个节点获取集群信息，存在disconnected连接状态节点则为不健康 checkAllNodeIfExistUnhealthyNodes(); Set\u0026lt;String\u0026gt; unhealthyNodes = redisNodeContext.getUnhealthyNodes(); if (unhealthyNodes.isEmpty()) { log.info(\u0026#34;No unhealthy nodes to refresh\u0026#34;); return; } log.info(\u0026#34;Starting topology refresh for {} unhealthy nodes: {}\u0026#34;, unhealthyNodes.size(), String.join(\u0026#34;,\u0026#34;, unhealthyNodes)); isRefreshing = true; try { Set\u0026lt;String\u0026gt; recoveredNodes = new HashSet\u0026lt;\u0026gt;(); for (String node : unhealthyNodes) { if (checkAndRefreshNode(node)) { recoveredNodes.add(node); } } // 处理恢复的节点 if (!recoveredNodes.isEmpty()) { handleRecoveredNodes(recoveredNodes); } log.info(\u0026#34;Topology refresh completed. Recovered {} nodes, Redis cluster refresh status:{}\u0026#34;, recoveredNodes.size(), getRefreshStatus()); } catch (Exception e) { log.error(\u0026#34;Error during topology refresh\u0026#34;, e); } finally { isRefreshing = false; } } private void checkAllNodeIfExistUnhealthyNodes() { Set\u0026lt;String\u0026gt; allNodes = redisNodeContext.getAllNodes(); String oneNode = allNodes.iterator().next(); String[] parts = oneNode.split(\u0026#34;:\u0026#34;); String host = parts[0]; int port = parts.length \u0026gt; 1 ? Integer.parseInt(parts[1]) : 6379; try (Jedis jedis = new Jedis(host, port, CONNECTION_TIMEOUT)) { jedis.auth(auth); String clusterNodes = jedis.clusterNodes(); log.info(\u0026#34;Redis cluster nodes : {}\u0026#34;,clusterNodes); String[] lines = clusterNodes.split(Const.Symbol.BlankLine); for(String line:lines){ if(line.trim().isEmpty()){ continue; } String[] lineParts = line.split(\u0026#34;\\\\s+\u0026#34;); if(lineParts.length\u0026lt;8){ continue; } String linkState = lineParts[7]; if(\u0026#34;disconnected\u0026#34;.equals(linkState)){ String ipPortPart = lineParts[1]; String node = ipPortPart.split(\u0026#34;@\u0026#34;)[0]; log.info(\u0026#34;Find unheanthy Redis node : {}\u0026#34;,node); redisNodeContext.addUnhealthyNode(node); } } } catch (Exception e) { log.error(\u0026#34;Exist unhealthy Redis node\u0026#34;, e); redisNodeContext.addUnhealthyNode(oneNode); } } /** * 检查并刷新单个节点 * * @param node * @return */ private boolean checkAndRefreshNode(String node) { String[] parts = node.split(\u0026#34;:\u0026#34;); String host = parts[0]; int port = parts.length \u0026gt; 1 ? Integer.parseInt(parts[1]) : 6379; log.info(\u0026#34;Checking node {}:{} for recovery\u0026#34;, host, port); // 首先检查节点是否恢复 if (!checkNodeAvailability(host, port)) { log.info(\u0026#34;Node {}:{} is still unavailable\u0026#34;, host, port); return false; } log.info(\u0026#34;Node {}:{} is available again, refreshing topology\u0026#34;, host, port); // 尝试刷新拓扑 boolean refreshSuccess = refreshNodeTopology(host, port); if (refreshSuccess) { log.info(\u0026#34;Successfully refreshed topology for recovered node: {}:{}\u0026#34;, host, port); return true; } else { log.warn(\u0026#34;Node {}:{} is available but topology refresh failed\u0026#34;, host, port); return false; } } /** * 检查节点是否可用 * * @param host * @param port * @return */ private boolean checkNodeAvailability(String host, int port) { try (Jedis jedis = new Jedis(host, port, CONNECTION_TIMEOUT)) { jedis.auth(auth); String result = jedis.ping(); return \u0026#34;PONG\u0026#34;.equals(result); } catch (Exception e) { log.warn(\u0026#34;Node {}:{} availability check failed: {}\u0026#34;, host, port, e.getMessage()); return false; } } /** * 刷新节点拓扑 * * @param host * @param port * @return */ private boolean refreshNodeTopology(String host, int port) { try (Jedis jedis = new Jedis(host, port, CONNECTION_TIMEOUT)) { jedis.auth(auth); // 验证节点确实是集群节点 String clusterInfo = jedis.clusterInfo(); if (!clusterInfo.contains(\u0026#34;cluster_state:ok\u0026#34;)) { log.warn(\u0026#34;Node {}:{} cluster state is not ok\u0026#34;, host, port); return false; } // 获取集群槽位信息 List\u0026lt;Object\u0026gt; slotInfo = jedis.clusterSlots(); if (slotInfo == null || slotInfo.isEmpty()) { log.warn(\u0026#34;Node {}:{} returned empty cluster slots\u0026#34;, host, port); return false; } log.info(\u0026#34;Node {}:{} cluster slots info retrieved successfully\u0026#34;, host, port); return topologyRefresh(jedis); } catch (Exception e) { log.error(\u0026#34;Failed to refresh topology for node {}:{}\u0026#34;, host, port, e); return false; } } /** * 拓扑刷新 * * @param jedis * @return */ private boolean topologyRefresh(Jedis jedis) { try { if (redisFactory instanceof JedisFactoryClusterImpl) { Object obj = redisFactory.getObject(); JedisCluster jedisCluster = (JedisCluster) obj; Field connectionHandlerField = BinaryJedisCluster.class.getDeclaredField(\u0026#34;connectionHandler\u0026#34;); connectionHandlerField.setAccessible(true); Object connectionHandler = connectionHandlerField.get(jedisCluster); if (connectionHandler instanceof JedisSlotBasedConnectionHandler) { JedisSlotBasedConnectionHandler handler = (JedisSlotBasedConnectionHandler) connectionHandler; handler.renewSlotCache(jedis); } } log.info(\u0026#34;Topology refresh triggered for recovered nodes\u0026#34;); return true; } catch (Exception e) { log.error(\u0026#34;Failed to trigger topology refresh\u0026#34;, e); return false; } } /** * 处理恢复的节点 * * @param recoveredNodes */ private void handleRecoveredNodes(Set\u0026lt;String\u0026gt; recoveredNodes) { // 从上下文中移除恢复的节点 for (String node : recoveredNodes) { redisNodeContext.removeUnhealthyNode(node); } log.info(\u0026#34;{} nodes recovered and removed from unhealthy list: {}\u0026#34;, recoveredNodes.size(), String.join(\u0026#34;,\u0026#34;, recoveredNodes)); } /** * 获取刷新状态报告 * * @return */ public Map\u0026lt;String, Object\u0026gt; getRefreshStatus() { Map\u0026lt;String, Object\u0026gt; status = new HashMap\u0026lt;\u0026gt;(); status.put(\u0026#34;unhealthyNodes\u0026#34;, redisNodeContext.getUnhealthyNodes()); status.put(\u0026#34;unhealthyNodeCount\u0026#34;, redisNodeContext.getUnhealthyNodeCount()); status.put(\u0026#34;isRefreshing\u0026#34;, isRefreshing); status.put(\u0026#34;lastRefreshTime\u0026#34;, new Date()); status.put(\u0026#34;refreshInterval\u0026#34;, REFRESH_INTERVAL); return status; } } 第二阶段：长效解决（拓扑管理） 方案：实现集群拓扑定期刷新机制\n拓扑刷新器设计： 刷新频率：每2分钟一次 刷新内容：集群节点状态、主从关系、slot分布 故障检测：自动识别并排除故障节点 架构改进： graph TB\rsubgraph \"拓扑刷新架构\"\rA[应用程序集群] --\u003e B[拓扑刷新服务]\rB --\u003e C[Redis集群]\rB --\u003e D[本地拓扑缓存]\rA --\u003e D\rend 关键特性： 异步刷新，不影响主业务流程 增量更新，减少网络开销 监控告警，及时发现刷新异常 经验教训与改进措施 配置管理改进 部署规范强化 强制约束：同一分片的主从节点必须跨数据中心部署 自动检查：部署时自动验证节点分布合理性 定期审计：每月进行集群配置合规性检查 配置版本控制 所有Redis配置变更纳入版本管理系统 演练操作必须包含回滚方案和验证步骤 框架升级与优化 客户端升级计划\ntimeline\rtitle Redis客户端升级路线图\rsection 短期\r优化现有Jedis 2.9 : 健康检查及拓扑刷新\rsection 中期\r评估并迁移到Jedis高版本 : 底层API不兼容\rsection 长期\r应用设计需考虑redis集群状态 : 使用本地缓存或数据库代替 健壮性增强\n实现状态检测连接策略 完善降级方案 监控与告警完善 新增监控指标\n1 2 3 4 5 # 监控指标定义 redis_cluster_health = Gauge(\u0026#39;redis_cluster_health\u0026#39;, \u0026#39;Redis集群健康状态\u0026#39;) redis_slot_coverage = Gauge(\u0026#39;redis_slot_coverage\u0026#39;, \u0026#39;Redis slot覆盖比例\u0026#39;) redis_node_distribution = Gauge(\u0026#39;redis_node_distribution\u0026#39;, \u0026#39;Redis节点分布合规性\u0026#39;) redis_topology_refresh_latency = Histogram(\u0026#39;redis_topology_refresh_latency\u0026#39;, \u0026#39;拓扑刷新延迟\u0026#39;) 告警规则优化\n主节点同机房告警 slot未覆盖告警 拓扑刷新失败告警 应急响应改进 预案完善 制定Redis集群部分不可用时的应急方案 明确降级场景和业务影响范围 演练常态化 每季度进行Redis故障切换演练 每年进行跨数据中心容灾演练 总结 本次事件暴露出我们在Redis集群管理中的多个薄弱环节，包括历史操作遗留问题、客户端框架局限性、配置与业务需求的矛盾等。通过本次事件的处理，我们不仅解决了当前问题，更重要的是建立了一套完整的预防和改进机制。\n核心启示：\n演练的真实性：演练必须尽可能模拟真实故障场景，并及时恢复 配置的生命周期管理：所有配置变更必须有跟踪、有审计、有回滚 客户端的健壮性：客户端必须能够处理集群部分故障的情况 监控的全面性：监控不仅要关注服务状态，还要关注架构合规性 我们将以此事件为契机，全面提升Redis集群的稳定性和可用性，确保金融业务的连续性和数据一致性。\n","date":"2025-12-16T00:00:00Z","image":"https://sturdy-space-chainsaw-595gwwvqgjvf4vpx-1313.app.github.dev/redis.jpeg","permalink":"https://sturdy-space-chainsaw-595gwwvqgjvf4vpx-1313.app.github.dev/p/redis%E7%94%9F%E4%BA%A7%E4%BA%8B%E4%BB%B6%E6%80%BB%E7%BB%93/","title":"Redis生产事件总结"}]